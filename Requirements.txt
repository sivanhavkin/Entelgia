Requirements-

Python 3.10+
Ollama with a local LLM (e.g., phi3, mistral, neural-chat)
8GB+ RAM recommended (16GB+ for larger models)

 
requests>=2.31.0 # HTTP requests to Ollama

colorama>=0.4.6 # Colored terminal output

python-dotenv>=1.0.0 # Environment variables from .env

API Server

fastapi>=0.104.0 # REST API framework

uvicorn>=0.24.0 # ASGI server

pydantic>=2.0.0 # Data validation

Testing

pytest>=7.4.0 # Testing framework

pytest-mock>=3.12.0 # Mocking for tests

Optional

python-dateutil>=2.8.2 # Date utilities

ðŸ”§ Installing Ollama (Required)
Entelgia runs entirely on a local LLM for privacy, control, and reproducibility. You must install Ollama before running the system.

Step 1: Download Ollama
Download Ollama for your operating system:

ðŸ‘‰ https://ollama.com

Supported platforms:

macOS
Linux
Windows (WSL recommended)
Step 2: Install a Model
After installing Ollama, pull at least one supported model:

ollama pull phi3
Recommended models:

phi3 (3.8B) â€“ Fast, low memory, ideal for testing
mistral (7B) â€“ Balanced reasoning and performance
neural-chat (7B) â€“ Strong conversational coherence
openchat (7B) â€“ Fast and stable dialogue
ðŸ’¡ On systems with 8GB RAM, prefer phi3. Larger models may be slow or unstable.

Step 3: Verify Ollama Is Running
Run a quick test:

ollama run phi3 "hello"
If you see a response, Ollama is installed and working correctly.

ðŸš€ Quick Start
git clone https://github.com/sivanhavkin/Entelgia.git

cd Entelgia

ollama serve-if ollama don't start automatically 

python Entelgia_production_meta.py
